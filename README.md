# DeepLearningProjects

## MNIST_softmax.ipynb:
Implementation of Batch Gradient Descent and Mini-Batch Gradient Descent for Softmax only using numpy functionality (scikit learn used only for loading the data and splitting it into train and test sets)

## backprop.ipynb:
Implementation of the back propagation algorithm for an arbitrary (fully connected) MLP with Softmax. Training on MNIST

## tensorflow-MLP-ipynb:
Implementation of computational graphs in TensorFlow by using only "low-level" functions of TensorFlow
